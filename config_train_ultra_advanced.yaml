# Ultra Advanced Training Configuration for IoTGuard IDS/IPS
# CIC-IoT-2023 Dataset - Optimized for Maximum Performance

dataset:
  merged_csv_path: "dataset/CSV/MERGED_CSV/"
  csv_pattern: "Merged*.csv"
  train_files: 12  # Reduced to avoid memory issues
  test_files: 4    # Keep balanced split
  val_files: 4     # Keep balanced split
  
data_processing:
  # Advanced sampling strategy for class balance
  benign_sampling: "all"  # Keep all benign samples
  attack_sampling_per_class: 48000 # Slightly increased for better patterns
  min_samples_per_class: 500  # Keep rare classes (Web-Based: 3400, BruteForce: 1836)
  
  # Feature engineering
  polynomial_features: false  # DISABLED to reduce memory usage
  polynomial_degree: 2
  interaction_features: true  # RE-ENABLED - important for attack patterns
  statistical_features: true
  time_based_features: true
  
  # Data cleaning
  remove_duplicates: true
  handle_missing: "median"  # Options: mean, median, drop
  outlier_method: "isolation_forest"  # Options: iqr, isolation_forest, none
  outlier_contamination: 0.05
  
  # Scaling and normalization
  scaler_type: "robust"  # Options: standard, robust, minmax

class_mapping:
  # Hierarchical grouping for better classification
  DDoS-DoS:
    - "DDOS-ACK_FRAGMENTATION"
    - "DDOS-HTTP_FLOOD"
    - "DDOS-ICMP_FLOOD"
    - "DDOS-ICMP_FRAGMENTATION"
    - "DDOS-PSHACK_FLOOD"
    - "DDOS-RSTFINFLOOD"
    - "DDOS-SYN_FLOOD"
    - "DDOS-SLOWLORIS"
    - "DDOS-SYNONYMOUSIP_FLOOD"
    - "DDOS-TCP_FLOOD"
    - "DDOS-UDP_FLOOD"
    - "DDOS-UDP_FRAGMENTATION"
    - "DOS-HTTP_FLOOD"
    - "DOS-SYN_FLOOD"
    - "DOS-TCP_FLOOD"
    - "DOS-UDP_FLOOD"
  
  Mirai:
    - "MIRAI-GREETH_FLOOD"
    - "MIRAI-GREIP_FLOOD"
    - "MIRAI-UDPPLAIN"
  
  Reconnaissance:
    - "RECON-HOSTDISCOVERY"
    - "RECON-OSSCAN"
    - "RECON-PINGSWEEP"
    - "RECON-PORTSCAN"
    - "VULNERABILITYSCAN"
  
  Spoofing:
    - "DNS_SPOOFING"
    - "MITM-ARPSPOOFING"
  
  Web-Based:
    - "BROWSERHIJACKING"
    - "COMMANDINJECTION"
    - "SQLINJECTION"
    - "XSS"
    - "UPLOADING_ATTACK"
    - "BACKDOOR_MALWARE"
  
  BruteForce:
    - "DICTIONARYBRUTEFORCE"
  
  Benign:
    - "BENIGN"

# Stage 1: Binary Classification (Benign vs Attack)
stage1_binary:
  models:
    lightgbm:
      enabled: true
      num_leaves: 95   # Balanced - not too deep to avoid overfitting
      max_depth: 12    # Moderate depth for efficiency
      learning_rate: 0.04  # Slightly faster learning
      n_estimators: 500    # Good balance
      min_child_samples: 12  # Lower to catch attack patterns
      subsample: 0.85
      colsample_bytree: 0.85
      reg_alpha: 0.01  # MINIMAL regularization - priority on recall
      reg_lambda: 0.01
      class_weight: "balanced"
      is_unbalance: true  # Force attention to minority class (attacks)
      
    xgboost:
      enabled: true
      max_depth: 13    # Moderate depth for memory efficiency
      learning_rate: 0.05  # Balanced learning rate
      n_estimators: 500    # Good balance
      min_child_weight: 1  # MINIMIZED to catch all attack patterns
      subsample: 0.85
      colsample_bytree: 0.85
      gamma: 0.01  # MINIMAL pruning - keep attack-detecting branches
      reg_alpha: 0.01
      reg_lambda: 0.1
      scale_pos_weight: 8.0  # INCREASED to 8.0 - further prioritize catching attacks
      
    catboost:
      enabled: true  # DISABLED to speed up training
      depth: 10
      learning_rate: 0.03
      iterations: 500
      l2_leaf_reg: 3.0
      border_count: 128
      random_strength: 0.5
      
    neural_network:
      enabled: true  # DISABLED to speed up training
      hidden_layers: [256, 128, 64, 32]
      dropout_rate: 0.3
      activation: "relu"
      optimizer: "adam"
      learning_rate: 0.001
      batch_size: 512
      epochs: 50
      early_stopping_patience: 10
  
  ensemble:
    method: "weighted_average"  # Options: weighted_average, stacking, voting
    weights:
      lightgbm: 0.25  # Slightly reduced to accommodate NN weight
      xgboost: 0.65   # Favor XGBoost for attack detection
      catboost: 0.00
      neural_network: 0.10  # Small contribution from NN
  
  balancing:
    method: "smote"  # Using pure SMOTE for maximum attack examples
    sampling_strategy: 0.85  # Generate more attack samples (85% of benign count)
    k_neighbors: 5  # Standard for stability
  
  # Threshold tuning parameters
  min_precision_for_threshold: 0.45  # Minimum precision to maintain when tuning threshold for recall

# Optional neural network (small, memory-friendly) for Stage 1 - can be enabled when desired
stage1_nn:
  enabled: true
  hidden_layers: [128, 64]
  dropout_rate: 0.25
  activation: "relu"
  learning_rate: 0.001
  batch_size: 512
  epochs: 12
  early_stopping_patience: 4

# Stage 2: Multi-Class Attack Classification
stage2_multiclass:
  models:
    lightgbm:
      enabled: true
      num_leaves: 95   # INCREASED for better multi-class separation
      max_depth: 14    # INCREASED for complex attack patterns
      learning_rate: 0.03
      n_estimators: 500  # INCREASED for better convergence
      min_child_samples: 15  # REDUCED to learn rare attack types better
      subsample: 0.85
      colsample_bytree: 0.85
      reg_alpha: 0.05  # REDUCED regularization
      reg_lambda: 0.05
      class_weight: "balanced"
      
    xgboost:
      enabled: true
      max_depth: 13    # INCREASED for better patterns
      learning_rate: 0.03
      n_estimators: 500  # INCREASED for better convergence
      min_child_weight: 1  # MINIMIZED to catch all attack types
      subsample: 0.85
      colsample_bytree: 0.85
      gamma: 0.05  # REDUCED for less pruning
      reg_alpha: 0.05
      reg_lambda: 0.5
      
    catboost:
      enabled: false  # DISABLED to speed up training
      depth: 10
      learning_rate: 0.02
      iterations: 600
      l2_leaf_reg: 3.0
      border_count: 128
      random_strength: 0.5
      class_weights: [1, 1, 1, 1, 1, 1]  # Will be auto-balanced
  
  ensemble:
    method: "weighted_average"
    weights:
      lightgbm: 0.50  # INCREASED - good for multi-class
      xgboost: 0.50   # BALANCED with LightGBM
      catboost: 0.00
  
  balancing:
    method: "smote_undersampling"  # SMOTE + RandomUnderSampler
    smote_strategy: "not majority"
    undersample_strategy: "auto"
    k_neighbors: 5

# Training Configuration
training:
  random_seed: 42
  use_gpu: false
  n_jobs: -1  # Use all CPU cores
  verbose: 1
  
  # Cross-validation
  use_cross_validation: false
  cv_folds: 5
  
  # Feature selection
  feature_selection: true
  feature_selection_method: "mutual_info"  # Options: mutual_info, chi2, recursive
  n_features_to_select: "auto"  # Or specify number
  
  # Model persistence
  save_intermediate: true
  checkpoint_frequency: 100  # Save every N iterations

# Output Configuration
output:
  models_dir: "models/train_ultra_advanced_models/"
  results_dir: "results/train_ultra_advanced_results/"
  logs_dir: "logs/"
  
  # Metadata
  generate_metadata: true
  metadata_filename: "model_metadata.json"
  
  # Visualizations
  generate_plots: true
  plot_format: "png"
  plot_dpi: 300
  
  # Reports
  generate_report: true
  report_format: "txt"  # Options: txt, html, pdf
  
  # Model files
  compression: true  # Use compression for pickle files
  save_feature_importance: true
  save_confusion_matrices: true
  save_roc_curves: true
  save_precision_recall_curves: true
